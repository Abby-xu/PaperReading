{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUMQlrHJ5tywbdeDl/PCOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abby-xu/PaperReading/blob/master/ResNet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Up\n",
        "- Import libraries\n",
        "- Set device\n",
        "- Mount drive"
      ],
      "metadata": {
        "id": "CuV9gFEy-2zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.hub import load_state_dict_from_url\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "4ZmqkpAgn8Ug"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "h1n1zVEB9FIM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QomHImb7-2Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet 18"
      ],
      "metadata": {
        "id": "w9rLE_c0-sQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet Child Model: Residual Block\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self,inplanes: int,planes: int,stride: int = 1,downsample = None) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes,planes, 3, stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, 3, stride=1,padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "FsO3JJ5polEL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self,layers: list, num_classes: int = 1000,zero_init_residual: bool = False) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, layers[0], stride=1) # 'stride = 1' since output won't change (keep as 56x56 after maxpool?)\n",
        "        self.layer2 = self._make_layer(128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(512, layers[3], stride=2)\n",
        "\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #self.do = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "\n",
        "    def _make_layer(self, planes: int, blocks: int,stride: int = 1) -> nn.Sequential:\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes, 1, stride=stride, bias=False),\n",
        "                                       nn.BatchNorm2d(planes))\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(self.inplanes, planes))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Decide the forward pass of the model\n",
        "\n",
        "        1. input: conv1, bn1, relu, maxpool\n",
        "        2. conv: layer1, layer2, layer3, layer4 (or stage)\n",
        "        3. output: avgpool & fc\n",
        "        '''\n",
        "\n",
        "        # input\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Conv\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Output\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        #x = self.do(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "9RjTv9U9opLT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18(num_classes: int = 1000, pretrained: bool = False, path= None, progress: bool = True) -> ResNet:\n",
        "    '''\n",
        "    if the pretrained is not defined, the model will be trained from official resnet18-f37072fd.pth\n",
        "    progress shows the download progress bar if True\n",
        "    '''\n",
        "\n",
        "    # [2, 2, 2, 2] for ReNet18 layers, [3, 4, 6, 3] for ResNet34 layers\n",
        "    model=ResNet([2, 2, 2, 2], num_classes, zero_init_residual= True)\n",
        "    if pretrained:\n",
        "        if not path:\n",
        "            pretrained_state_dict = load_state_dict_from_url('https://download.pytorch.org/models/resnet18-f37072fd.pth',progress=progress)\n",
        "            print('load pretrained model')\n",
        "        else:\n",
        "            pretrained_state_dict = torch.load(path)\n",
        "        state_dict=model.state_dict()\n",
        "        for k in pretrained_state_dict:\n",
        "            if k in state_dict and k not in ['fc.weight', 'fc.bias']:\n",
        "                state_dict[k] = pretrained_state_dict[k].data\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "metadata": {
        "id": "aW_r9PHnossC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Model"
      ],
      "metadata": {
        "id": "_EBqhXIT_Yev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(8, 3, 80, 80)\n",
        "model = resnet18(2,pretrained=True)\n",
        "# model = resnet18(2)\n",
        "print(model)\n",
        "a = model.state_dict()\n",
        "o=model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o174hZUcovPp",
        "outputId": "17a9d81a-2966-40fe-e08b-a526143b46db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 181MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load pretrained model\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "jd67ZePx_cQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-train - data loading"
      ],
      "metadata": {
        "id": "Yw_IVES2_rNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset(batch_size=16,valid_size=0.2,num_workers=0,pic_path='dataset'):\n",
        "    \"\"\"\n",
        "    batch_size: Number of loaded drawings per batch\n",
        "    valid_size: Percentage of training set to use as validation\n",
        "    num_workers: Number of subprocesses to use for data loading\n",
        "    pic_path: The path of the pictrues\n",
        "    \"\"\"\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    # transfer data to torch.FloatTensor and standardized\n",
        "    train_data = datasets.CIFAR10(pic_path, train=True, download=True, transform=transform_train)\n",
        "    valid_data = datasets.CIFAR10(pic_path, train=True, download=True, transform=transform_test)\n",
        "    test_data = datasets.CIFAR10(pic_path, train=False, download=True, transform=transform_test)\n",
        "\n",
        "    # obtain training indices that will be used for validation\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    # random indices\n",
        "    np.random.shuffle(indices)\n",
        "    # the ratio of split\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    # divide data to radin_data and valid_data\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    # prepare data loaders (combine dataset and sampler)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "        sampler=train_sampler, num_workers=num_workers)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size,\n",
        "        sampler=valid_sampler, num_workers=num_workers)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "        num_workers=num_workers)\n",
        "\n",
        "    return train_loader,valid_loader,test_loader"
      ],
      "metadata": {
        "id": "PHjty8mN9BMl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_loader,valid_loader,test_loader = read_dataset(batch_size=batch_size,pic_path='dataset')\n",
        "\n",
        "# load model - non-pretrained\n",
        "n_class = 10\n",
        "model = resnet18()\n",
        "model = model.to(device)\n",
        "# using cross entropy loss func\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# start training\n",
        "n_epochs = 50\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "accuracy = []\n",
        "lr = 0.1\n",
        "counter = 0\n",
        "for epoch in tqdm(range(1, n_epochs+1)):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    total_sample = 0\n",
        "    right_sample = 0\n",
        "\n",
        "    if counter/10 ==1:\n",
        "        counter = 0\n",
        "        lr = lr*0.5\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    ##### model of training set #####\n",
        "    model.train() # using batch normalization and drop out\n",
        "    for data, target in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data).to(device)  # == 'output = model.forward(data).to(device)'\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "    ##### model of validation set #####\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data).to(device)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare predictions to true label\n",
        "        correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "        # correct = np.squeeze(correct_tensor.to(device).numpy())\n",
        "        total_sample += batch_size\n",
        "        for i in correct_tensor:\n",
        "            if i:\n",
        "                right_sample += 1\n",
        "    print(\"\\nAccuracy:\",100*right_sample/total_sample,\"%\")\n",
        "    accuracy.append(right_sample/total_sample)\n",
        "\n",
        "    # cal avg loss\n",
        "    train_loss = train_loss/len(train_loader.sampler)\n",
        "    valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "\n",
        "    # save model\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).'.format(valid_loss_min,valid_loss))\n",
        "        # print('Saving model ...')\n",
        "        # torch.save(model.state_dict(), 'checkpoint/resnet18_cifar10.pt')\n",
        "        valid_loss_min = valid_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "    print('------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9w9hP3a_J1d",
        "outputId": "8a23010c-a0c2-44c4-b411-591e18ac440b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/50 [00:24<19:49, 24.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 52.35363924050633 %\n",
            "Epoch: 1 \tTraining Loss: 1.595356 \tValidation Loss: 1.319011\n",
            "Validation loss decreased (inf --> 1.319011).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 2/50 [00:48<19:26, 24.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 60.68037974683544 %\n",
            "Epoch: 2 \tTraining Loss: 1.079899 \tValidation Loss: 1.111023\n",
            "Validation loss decreased (1.319011 --> 1.111023).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/50 [01:12<19:02, 24.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 65.92167721518987 %\n",
            "Epoch: 3 \tTraining Loss: 0.864617 \tValidation Loss: 0.958296\n",
            "Validation loss decreased (1.111023 --> 0.958296).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/50 [01:37<18:41, 24.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 68.40387658227849 %\n",
            "Epoch: 4 \tTraining Loss: 0.747217 \tValidation Loss: 0.885348\n",
            "Validation loss decreased (0.958296 --> 0.885348).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 5/50 [02:01<18:15, 24.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 69.48180379746836 %\n",
            "Epoch: 5 \tTraining Loss: 0.654395 \tValidation Loss: 0.866201\n",
            "Validation loss decreased (0.885348 --> 0.866201).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/50 [02:25<17:49, 24.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 70.44106012658227 %\n",
            "Epoch: 6 \tTraining Loss: 0.582278 \tValidation Loss: 0.865008\n",
            "Validation loss decreased (0.866201 --> 0.865008).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 7/50 [02:50<17:26, 24.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 69.10601265822785 %\n",
            "Epoch: 7 \tTraining Loss: 0.528030 \tValidation Loss: 0.911217\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|█▌        | 8/50 [03:14<17:01, 24.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 71.67721518987342 %\n",
            "Epoch: 8 \tTraining Loss: 0.478582 \tValidation Loss: 0.836266\n",
            "Validation loss decreased (0.865008 --> 0.836266).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 9/50 [03:39<16:47, 24.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 70.18393987341773 %\n",
            "Epoch: 9 \tTraining Loss: 0.435760 \tValidation Loss: 0.929269\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 10/50 [04:04<16:22, 24.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 72.37935126582279 %\n",
            "Epoch: 10 \tTraining Loss: 0.392269 \tValidation Loss: 0.831641\n",
            "Validation loss decreased (0.836266 --> 0.831641).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 11/50 [04:29<16:02, 24.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 69.61036392405063 %\n",
            "Epoch: 11 \tTraining Loss: 0.366163 \tValidation Loss: 0.988677\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 12/50 [04:53<15:34, 24.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 69.61036392405063 %\n",
            "Epoch: 12 \tTraining Loss: 0.342061 \tValidation Loss: 1.011468\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|██▌       | 13/50 [05:18<15:07, 24.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 71.88488924050633 %\n",
            "Epoch: 13 \tTraining Loss: 0.320673 \tValidation Loss: 0.897379\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 14/50 [05:42<14:40, 24.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 70.74762658227849 %\n",
            "Epoch: 14 \tTraining Loss: 0.297359 \tValidation Loss: 0.937570\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 15/50 [06:06<14:17, 24.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 71.74643987341773 %\n",
            "Epoch: 15 \tTraining Loss: 0.293721 \tValidation Loss: 0.951103\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 16/50 [06:31<13:50, 24.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 72.8935917721519 %\n",
            "Epoch: 16 \tTraining Loss: 0.272644 \tValidation Loss: 0.900356\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|███▍      | 17/50 [06:55<13:23, 24.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 72.01344936708861 %\n",
            "Epoch: 17 \tTraining Loss: 0.275835 \tValidation Loss: 0.955970\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 18/50 [07:19<13:00, 24.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 71.41020569620254 %\n",
            "Epoch: 18 \tTraining Loss: 0.265940 \tValidation Loss: 0.931372\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 19/50 [07:44<12:36, 24.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 71.4003164556962 %\n",
            "Epoch: 19 \tTraining Loss: 0.253923 \tValidation Loss: 0.949594\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 20/50 [08:08<12:10, 24.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 71.20253164556962 %\n",
            "Epoch: 20 \tTraining Loss: 0.253392 \tValidation Loss: 1.020884\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 21/50 [08:32<11:45, 24.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 76.78995253164557 %\n",
            "Epoch: 21 \tTraining Loss: 0.069627 \tValidation Loss: 0.905729\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 22/50 [08:57<11:23, 24.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 77.6503164556962 %\n",
            "Epoch: 22 \tTraining Loss: 0.023470 \tValidation Loss: 0.910688\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 23/50 [09:21<10:57, 24.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 75.77136075949367 %\n",
            "Epoch: 23 \tTraining Loss: 0.018261 \tValidation Loss: 1.076827\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 24/50 [09:45<10:34, 24.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 72.30023734177215 %\n",
            "Epoch: 24 \tTraining Loss: 0.074131 \tValidation Loss: 1.115674\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 25/50 [10:10<10:07, 24.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 73.05181962025317 %\n",
            "Epoch: 25 \tTraining Loss: 0.112248 \tValidation Loss: 1.038243\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 26/50 [10:34<09:43, 24.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 74.53520569620254 %\n",
            "Epoch: 26 \tTraining Loss: 0.123569 \tValidation Loss: 0.940431\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 27/50 [10:58<09:19, 24.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 74.1198575949367 %\n",
            "Epoch: 27 \tTraining Loss: 0.113089 \tValidation Loss: 0.975857\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 28/50 [11:23<08:57, 24.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 74.42642405063292 %\n",
            "Epoch: 28 \tTraining Loss: 0.121116 \tValidation Loss: 0.970545\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 29/50 [11:47<08:32, 24.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 74.66376582278481 %\n",
            "Epoch: 29 \tTraining Loss: 0.124784 \tValidation Loss: 0.993191\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 30/50 [12:12<08:07, 24.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 73.15071202531645 %\n",
            "Epoch: 30 \tTraining Loss: 0.106638 \tValidation Loss: 1.060766\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 31/50 [12:36<07:42, 24.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 78.74802215189874 %\n",
            "Epoch: 31 \tTraining Loss: 0.024569 \tValidation Loss: 0.851826\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 32/50 [13:01<07:20, 24.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.00395569620254 %\n",
            "Epoch: 32 \tTraining Loss: 0.003937 \tValidation Loss: 0.811809\n",
            "Validation loss decreased (0.831641 --> 0.811809).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 33/50 [13:26<06:59, 24.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.14240506329114 %\n",
            "Epoch: 33 \tTraining Loss: 0.001773 \tValidation Loss: 0.794922\n",
            "Validation loss decreased (0.811809 --> 0.794922).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 34/50 [13:51<06:37, 24.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.20174050632912 %\n",
            "Epoch: 34 \tTraining Loss: 0.001421 \tValidation Loss: 0.773721\n",
            "Validation loss decreased (0.794922 --> 0.773721).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 35/50 [14:16<06:12, 24.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.32041139240506 %\n",
            "Epoch: 35 \tTraining Loss: 0.001402 \tValidation Loss: 0.767851\n",
            "Validation loss decreased (0.773721 --> 0.767851).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 36/50 [14:40<05:45, 24.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.24129746835443 %\n",
            "Epoch: 36 \tTraining Loss: 0.001322 \tValidation Loss: 0.760757\n",
            "Validation loss decreased (0.767851 --> 0.760757).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 37/50 [15:05<05:20, 24.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.32041139240506 %\n",
            "Epoch: 37 \tTraining Loss: 0.001426 \tValidation Loss: 0.755303\n",
            "Validation loss decreased (0.760757 --> 0.755303).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 38/50 [15:29<04:54, 24.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.32041139240506 %\n",
            "Epoch: 38 \tTraining Loss: 0.001445 \tValidation Loss: 0.752183\n",
            "Validation loss decreased (0.755303 --> 0.752183).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 39/50 [15:54<04:29, 24.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.32041139240506 %\n",
            "Epoch: 39 \tTraining Loss: 0.001386 \tValidation Loss: 0.748416\n",
            "Validation loss decreased (0.752183 --> 0.748416).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 40/50 [16:18<04:05, 24.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.46875 %\n",
            "Epoch: 40 \tTraining Loss: 0.001378 \tValidation Loss: 0.747692\n",
            "Validation loss decreased (0.748416 --> 0.747692).\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 41/50 [16:43<03:40, 24.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.37974683544304 %\n",
            "Epoch: 41 \tTraining Loss: 0.001355 \tValidation Loss: 0.752182\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 42/50 [17:07<03:16, 24.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.35996835443038 %\n",
            "Epoch: 42 \tTraining Loss: 0.001337 \tValidation Loss: 0.750868\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 43/50 [17:32<02:52, 24.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.28085443037975 %\n",
            "Epoch: 43 \tTraining Loss: 0.001323 \tValidation Loss: 0.754184\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 44/50 [17:57<02:28, 24.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.46875 %\n",
            "Epoch: 44 \tTraining Loss: 0.001293 \tValidation Loss: 0.748960\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 45/50 [18:23<02:04, 24.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.55775316455696 %\n",
            "Epoch: 45 \tTraining Loss: 0.001265 \tValidation Loss: 0.750366\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 46/50 [18:47<01:39, 24.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.47863924050633 %\n",
            "Epoch: 46 \tTraining Loss: 0.001259 \tValidation Loss: 0.757070\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 47/50 [19:12<01:14, 24.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.55775316455696 %\n",
            "Epoch: 47 \tTraining Loss: 0.001217 \tValidation Loss: 0.756305\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 48/50 [19:37<00:49, 24.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.61708860759494 %\n",
            "Epoch: 48 \tTraining Loss: 0.001215 \tValidation Loss: 0.760608\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 49/50 [20:01<00:24, 24.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.43908227848101 %\n",
            "Epoch: 49 \tTraining Loss: 0.001200 \tValidation Loss: 0.761368\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [20:26<00:00, 24.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 80.48852848101266 %\n",
            "Epoch: 50 \tTraining Loss: 0.001170 \tValidation Loss: 0.767953\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}